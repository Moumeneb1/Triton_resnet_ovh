{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirments \n",
    "#!cp -r /models .\n",
    "#pip install tritonclient['all']\n",
    "#pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy models on ovhai using Triton\n",
    "In this section, we will use the ovhai tutorial to demonstrate how to deploy machine learning models on the ovhai cloud using Triton. \n",
    "You must first import the same storage container from which we exported part one models (onnx/jit/torch), and then configure the config files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"pytorch-model-gpu\"\n",
    "platform: \"pytorch_libtorch\"\n",
    "max_batch_size: 32\n",
    "instance_group [\n",
    "    {\n",
    "      count: 1\n",
    "      kind: KIND_GPU\n",
    "    }\n",
    "  ]\n",
    "input [\n",
    " {\n",
    "    name: \"input\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "with open('models/pytorch-model-gpu/config.pbtxt', 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"onnx-model-gpu\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "max_batch_size: 32\n",
    "instance_group [\n",
    "    {\n",
    "      count: 1\n",
    "      kind: KIND_GPU\n",
    "    }\n",
    "  ]\n",
    "input [\n",
    " {\n",
    "    name: \"input\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "with open('models/onnx-model-gpu/config.pbtxt', 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = \"\"\"\n",
    "name: \"tensorrt-model\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 32\n",
    "instance_group [\n",
    "    {\n",
    "      count: 1\n",
    "      kind: KIND_GPU\n",
    "    }\n",
    "  ]\n",
    "input [\n",
    " {\n",
    "    name: \"input\"\n",
    "    data_type: TYPE_FP32\n",
    "    format: FORMAT_NCHW\n",
    "    dims: [ 3, 224, 224 ]\n",
    "  }\n",
    "]\n",
    "output {\n",
    "    name: \"output\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ 1000 ]\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "with open('models/tensorrt-model/config.pbtxt', 'w') as file:\n",
    "    file.write(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the Triton server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tritonserver --model-repository=/workspace/models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send requests using tritonclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries  and initialize Triton variables. \n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import tritonhttpclient\n",
    "import tritongrpcclient\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "#  Set according to your config, default is localhost (http_url= 'localhost:8000' / grpc_url = 'localhost:8001')\n",
    "http_url = 'localhost:8000'\n",
    "grpc_url = 'localhost:8001'\n",
    "verbose = False\n",
    "concurrency = 100\n",
    "model_version = '1'\n",
    "batch_size = 1\n",
    "triton_http_client = tritonhttpclient.InferenceServerClient(url=http_url, verbose=verbose)\n",
    "triton_grpc_client = tritongrpcclient.InferenceServerClient(url=grpc_url, verbose=verbose)\n",
    "\n",
    "input_dtype = 'FP32'\n",
    "input_name = 'input'\n",
    "input_shape = (1, 3, 224, 224)\n",
    "output_name = 'output'\n",
    "\n",
    "input0 = tritonhttpclient.InferInput(input_name, input_shape, input_dtype)\n",
    "dummy_data = np.ones(shape=input_shape, dtype=np.float32)\n",
    "input0.set_data_from_numpy(dummy_data, binary_data=True)\n",
    "output = tritonhttpclient.InferRequestedOutput(output_name, binary_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:09<00:00, 104.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onnx-model-gpu\n",
      "Average Latency: ~0.009536562204360962 seconds\n",
      "Average Throughput: ~104.85958971071476 examples / second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# run ONNX inferences\n",
    "model_name = 'onnx-model-gpu'\n",
    "requests = []\n",
    "request_count = 1000\n",
    "\n",
    "start_time = time.time()\n",
    "for i in tqdm(range(request_count)):\n",
    "    requests.append(triton_http_client.infer(model_name, model_version=model_version, inputs=[input0], outputs=[output]))\n",
    "end_time = time.time()\n",
    "\n",
    "print(model_name)\n",
    "print('Average Latency: ~{} seconds'.format((end_time - start_time) / request_count))\n",
    "print('Average Throughput: ~{} examples / second'.format(batch_size * request_count / (end_time - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:06<00:00, 157.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorrt-model\n",
      "Average Latency: ~0.006342987060546875 seconds\n",
      "Average Throughput: ~157.65442849788548 examples / second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# run TorchScript inferences\n",
    "model_name = 'tensorrt-model'\n",
    "requests = []\n",
    "request_count = 1000\n",
    "\n",
    "start_time = time.time()\n",
    "for i in tqdm(range(request_count)):\n",
    "    requests.append(triton_http_client.infer(model_name, model_version=model_version, inputs=[input0], outputs=[output]))\n",
    "end_time = time.time()\n",
    "\n",
    "print(model_name)\n",
    "print('Average Latency: ~{} seconds'.format((end_time - start_time) / request_count))\n",
    "print('Average Throughput: ~{} examples / second'.format(batch_size * request_count / (end_time - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:17<00:00, 58.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch-model-gpu\n",
      "Average Latency: ~0.0170787250995636 seconds\n",
      "Average Throughput: ~58.55237988610475 examples / second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries  and initialize Triton variables. \n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import tritonhttpclient\n",
    "import tritongrpcclient\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "#  Set according to your config, default is localhost (http_url= 'localhost:8000' / grpc_url = 'localhost:8001')\n",
    "http_url = 'localhost:8000'\n",
    "grpc_url = 'localhost:8001'\n",
    "verbose = False\n",
    "concurrency = 100\n",
    "model_version = '1'\n",
    "batch_size = 1\n",
    "triton_http_client = tritonhttpclient.InferenceServerClient(url=http_url, verbose=verbose)\n",
    "triton_grpc_client = tritongrpcclient.InferenceServerClient(url=grpc_url, verbose=verbose)\n",
    "\n",
    "input_dtype = 'FP32'\n",
    "input_name = 'input__0'\n",
    "input_shape = (1, 3, 224, 224)\n",
    "output_name = 'output__0'\n",
    "\n",
    "input0 = tritonhttpclient.InferInput(input_name, input_shape, input_dtype)\n",
    "dummy_data = np.ones(shape=input_shape, dtype=np.float32)\n",
    "input0.set_data_from_numpy(dummy_data, binary_data=True)\n",
    "output = tritonhttpclient.InferRequestedOutput(output_name, binary_data=True)\n",
    "\n",
    "# run ONNX inferences\n",
    "model_name = 'pytorch-model-gpu'\n",
    "requests = []\n",
    "request_count = 1000\n",
    "\n",
    "start_time = time.time()\n",
    "for i in tqdm(range(request_count)):\n",
    "    requests.append(triton_http_client.infer(model_name, model_version=model_version, inputs=[input0], outputs=[output]))\n",
    "end_time = time.time()\n",
    "\n",
    "print(model_name)\n",
    "print('Average Latency: ~{} seconds'.format((end_time - start_time) / request_count))\n",
    "print('Average Throughput: ~{} examples / second'.format(batch_size * request_count / (end_time - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the Flask server \n",
    "Launch the server from a terminal using \n",
    "* python flask_app/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark the Flask server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run simple Flask Server\n",
    "import requests\n",
    "\n",
    "data = {'arr': dummy_data.tolist()}\n",
    "response = requests.post(\"http://127.0.0.1:5000/predict\", json=data)\n",
    "\n",
    "start_time = time.time()\n",
    "request_count = 1000\n",
    "\n",
    "for i in tqdm(range(request_count)):\n",
    "    response = requests.post(\"http://127.0.0.1:5000/predict\", json=data)\n",
    "end_time = time.time()\n",
    "\n",
    "print('simple flask')\n",
    "print('Average Latency: ~{} seconds'.format((end_time - start_time) / request_count))\n",
    "print('Average Throughput: ~{} examples / second'.format(batch_size * request_count / (end_time - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Perf Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: ./bin/perf_analyzer: No such file or directory\n",
      "/bin/bash: ./bin/perf_analyzer: No such file or directory\n",
      "E0319 13:43:58.253928372 1656448 backup_poller.cc:133]       Run client channel backup poller: {\"created\":\"@1647697438.253808720\",\"description\":\"pollset_work\",\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":320,\"referenced_errors\":[{\"created\":\"@1647697438.253800479\",\"description\":\"Bad file descriptor\",\"errno\":9,\"file\":\"src/core/lib/iomgr/ev_epollex_linux.cc\",\"file_line\":950,\"os_error\":\"Bad file descriptor\",\"syscall\":\"epoll_wait\"}]}\n",
      "/bin/bash: ./bin/perf_analyzer: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/triton-inference-server/server/releases/download/v2.19.0/v2.19.0_ubuntu2004.clients.tar.gz\n",
    "!tar -xvf \n",
    "!./bin/perf_analyzer -m onnx-model-gpu\n",
    "!./bin/perf_analyzer -m pytorch-model-gpu\n",
    "!./bin/perf_analyzer -m tensorrt-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request Triton prometheus metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -v 127.0.0.1:8002/metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
